{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,re\n",
    "import yt_dlp\n",
    "import json\n",
    "import time\n",
    "import math \n",
    "import httplib2\n",
    "import requests\n",
    "import pinecone \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import matplotlib.pyplot as plt\n",
    "from youtubesearchpython import *\n",
    "from langchain.llms import OpenAIChat\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chains import VectorDBQAWithSourcesChain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Besties GPT\n",
    "\n",
    "`Here, we will prepare the VectorDB index for ALL IN podcast:`\n",
    "\n",
    "* Use Whisper to transcribe episodes \n",
    "* Chunk data\n",
    "* Embed it to Pinecone\n",
    "* Test VectorDBQA chain on it \n",
    " \n",
    "`1. Get video urls -` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtubesearchpython import ChannelsSearch\n",
    "channelsSearch = ChannelsSearch('All In Podcast', limit = 10, region = 'US')\n",
    "print(channelsSearch.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/youtube-search-python/\n",
    "channel_id = \"UCESLZhusAkFfsNsApnjF_Cg\" # Get ID from ChannelsSearch\n",
    "playlist = Playlist(playlist_from_channel_id(channel_id))\n",
    "while playlist.hasMoreVideos:\n",
    "    print('Getting more videos...')\n",
    "    playlist.getNextVideos()\n",
    "    print(f'Videos Retrieved: {len(playlist.videos)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Episode data\n",
    "stor_metadata=pd.DataFrame()\n",
    "for v in playlist.videos:\n",
    "    try:\n",
    "        ep_number = int(v['title'].split(\":\")[0].split(\"E\")[-1])\n",
    "        stor_metadata.loc[v['title'],'number']=ep_number\n",
    "        stor_metadata.loc[v['title'],'link']=v['link']\n",
    "        stor_metadata.loc[v['title'],'title']=v['title']\n",
    "        stor_metadata.loc[v['title'],'img']=v['thumbnails'][3]['url']\n",
    "    except:\n",
    "        if v['title']==\"E76.5: Food shortage, China's grand plan, inflation, French election plus an All-In Summit preview\":\n",
    "            stor_metadata.loc[v['title'],'number']=1\n",
    "            stor_metadata.loc[v['title'],'link']=v['link']\n",
    "            stor_metadata.loc[v['title'],'title']=v['title']\n",
    "            stor_metadata.loc[v['title'],'img']=v['thumbnails'][3]['url']\n",
    "        print(\"Failed on %s\", v['title'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2. Get audio -` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through episodes \n",
    "for ix in stor_metadata.index[-95:]:\n",
    "    \n",
    "    ep_number=int(stor_metadata.loc[ix,'number'])\n",
    "    print(\"EPISODE: %s\"%ep_number)\n",
    "    img_url=stor_metadata.loc[ix,'img']\n",
    "    ep_link=stor_metadata.loc[ix,'link']\n",
    "    # Write img \n",
    "    with open(\"../public/0%s.jpg\"%str(ep_number), 'wb') as f:\n",
    "        response = requests.get(img_url)\n",
    "        f.write(response.content)\n",
    "    # Write audio\n",
    "    ydl_opts = {\n",
    "    'format': 'm4a/bestaudio/best',\n",
    "    'outtmpl': 'audio/%s.m4a'%str(ep_number),\n",
    "    'noplaylist': True,\n",
    "    'postprocessors': [{  \n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'm4a',\n",
    "    }]}\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        error_code = ydl.download(ep_link)\n",
    "        \n",
    "stor_metadata.reset_index().to_csv(\"audio_transcription/episodes.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3. Run Whisper -`\n",
    " \n",
    "* On GPU, ideally: 10-20 min / video on 2080Ti with `medium` model\n",
    "* Run `python run_whisper.py`\n",
    "\n",
    "If running this step on a remote machine:\n",
    "* scp the transcription: `audio_transcription/episodes.csv`\n",
    "* scp the audio files: `audio/*`\n",
    "* Run `python run_whisper.py`\n",
    "* Then, scp the `audio_transcription/` back to local "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python run_whisper.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4. Get transcripts -`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Chunk size: key parameter *** \n",
    "chunks = 1500\n",
    "splits_new = [ ]\n",
    "metadatas_new = [ ]\n",
    "\n",
    "# Read the csv file\n",
    "new_ep=pd.read_csv(\"audio_transcription/episodes.csv\",index_col=None)\n",
    "\n",
    "for ix in new_ep.index:\n",
    "\n",
    "    # Get data\n",
    "    title=new_ep.loc[ix,'title']\n",
    "    ep_number=int(new_ep.loc[ix,'number'])\n",
    "    \n",
    "    # Ep\n",
    "    episode_id=\"0\"+str(ep_number) \n",
    "    file_path='audio_transcription/%s.txt'%str(episode_id)\n",
    "    transcript=pd.read_csv(file_path,sep='\\t',header=None)\n",
    "    transcript.columns=['links','time','chunks']\n",
    "    \n",
    "    # Clean text chunks \n",
    "    transcript['clean_chunks']=transcript['chunks'].astype(str).apply(lambda x: x.strip())\n",
    "    links = list(transcript['links'])\n",
    "    texts = transcript['clean_chunks'].str.cat(sep=' ')\n",
    "\n",
    "    # Splits \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunks, \n",
    "                                                   chunk_overlap=50) \n",
    "    splits = text_splitter.split_text(texts)\n",
    "    print(len(splits)) \n",
    "\n",
    "    # Metadata \n",
    "    N = len(splits) \n",
    "    bins = np.linspace(0, len(links)-1, N, dtype=int)\n",
    "    sampled_links = [links[i] for i in bins]\n",
    "    \n",
    "    # Here we can add \"link\", \"title\", etc that can be fetched in the app \n",
    "    metadatas=[{\"source\":title + \" \" +link,\"id\":episode_id,\"link\":link,\"title\":title} for link in sampled_links]\n",
    "    print(len(metadatas)) \n",
    "\n",
    "    # Append to output \n",
    "    splits_new.append(splits)\n",
    "    metadatas_new.append(metadatas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5. Assemble final list -`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the list of lists \n",
    "splits_all = []\n",
    "for sublist in splits_new:\n",
    "    splits_all.extend(sublist)\n",
    "\n",
    "metadatas_all = []\n",
    "for sublist in metadatas_new:\n",
    "    metadatas_all.extend(sublist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`6. Embed full dataset in Pinecone VectorDB -`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone\n",
    "pinecone.init(\n",
    "    api_key=os.environ.get('PINECONE_API_KEY'),  \n",
    "    environment=\"us-east1-gcp\")\n",
    "\n",
    "# Update - \n",
    "index_name = \"besties-gpt\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "p = Pinecone.from_existing_index(index_name=index_name,embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data in chunk to avoid data ingest errors\n",
    "chunk_size = 100\n",
    "last_chunk = 0\n",
    "num_chunks = math.ceil(len(splits_all) / chunk_size)\n",
    "for i in range(last_chunk,num_chunks):\n",
    "    \n",
    "    print(i)\n",
    "    start_time = time.time()\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min(start_idx + chunk_size, len(splits_all))\n",
    "    \n",
    "    # Extract the current chunk\n",
    "    current_splits = splits_all[start_idx:end_idx]\n",
    "    current_metadatas = metadatas_all[start_idx:end_idx]\n",
    "    \n",
    "    # Add the current chunk to the vector database\n",
    "    p.add_texts(texts = current_splits, metadatas=current_metadatas)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    print(\"--------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`7. Read in VectorDB for testing` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone\n",
    "pinecone.init(\n",
    "    api_key=os.environ.get('PINECONE_API_KEY'),  \n",
    "    environment=\"us-east1-gcp\")\n",
    "index_name = \"besties-gpt\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "p = Pinecone.from_existing_index(index_name=index_name,embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_retrievalQA_sources_chain(llm,query,docstore):\n",
    "\n",
    "    start_time = time.time()\n",
    "    chain = RetrievalQAWithSourcesChain.from_chain_type(llm,chain_type=\"stuff\",retriever=docstore.as_retriever(k=3))\n",
    "    a = chain({\"question\": query},return_only_outputs=True)\n",
    "    print(a[\"answer\"])\n",
    "    print(a[\"sources\"])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    print(\"--------\")\n",
    "\n",
    "def run_vectorDBQA_sources_chain(llm,query,docstore,k):\n",
    "\n",
    "    start_time = time.time()\n",
    "    chain = VectorDBQAWithSourcesChain.from_chain_type(llm,chain_type=\"stuff\",vectorstore=docstore,k=k)\n",
    "    a = chain({\"question\": query},return_only_outputs=True)\n",
    "    print(a[\"answer\"])\n",
    "    print(a[\"sources\"])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    print(\"--------\")\n",
    "\n",
    "llm = OpenAIChat(temperature=0)\n",
    "q = \"What is the cause of the SVB crisis?\"\n",
    "run_vectorDBQA_sources_chain(llm,q,p,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAIChat(model_name=\"gpt-4\",temperature=0)\n",
    "q = \"What is the cause of the SVB crisis?\"\n",
    "run_vectorDBQA_sources_chain(llm,q,p,8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`8. Evaluation` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('eval/final_eval.json', 'r') as f:\n",
    "    eval_set = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import VectorDBQA\n",
    "\n",
    "llm = OpenAIChat(temperature=0)\n",
    "chain_gpt3_k_4 = VectorDBQA.from_chain_type(llm,chain_type=\"stuff\",vectorstore=p,k=4,input_key = \"question\")\n",
    "\n",
    "llm = OpenAIChat(model_name=\"gpt-4\",temperature=0)\n",
    "chain_gpt4_k_4 = VectorDBQA.from_chain_type(llm,chain_type=\"stuff\",vectorstore=p,k=4,input_key = \"question\")\n",
    "chain_gpt4_k_8 = VectorDBQA.from_chain_type(llm,chain_type=\"stuff\",vectorstore=p,k=8,input_key = \"question\")\n",
    " \n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from langchain.chat_models import ChatOpenAI \n",
    "eval_chain = QAEvalChain.from_llm(llm=ChatOpenAI(temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(chain):\n",
    "\n",
    "    predictions = []\n",
    "    predicted_dataset = []\n",
    "    latency = []\n",
    "\n",
    "    for data in eval_set:\n",
    "        \n",
    "        start_time = time.time()\n",
    "        new_data = {\"question\": data[\"question\"],\"answer\": data[\"answer\"]}\n",
    "        predictions.append(chain(new_data))\n",
    "        predicted_dataset.append(new_data)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        latency.append(elapsed_time)\n",
    "\n",
    "    return predictions,predicted_dataset,latency\n",
    "\n",
    "predictions_list = []\n",
    "scores_list = []\n",
    "latency_list = []\n",
    "\n",
    "# Eval on chains \n",
    "for i,chain in enumerate([chain_gpt3_k_4,chain_gpt4_k_4,chain_gpt4_k_8]):    \n",
    "    print(f\"Evaluating chain {i+1}\")\n",
    "    predictions,predicted_dataset,latency=run_eval(chain)\n",
    "    predictions_list.append(predictions)\n",
    "    graded_outputs = eval_chain.evaluate(predicted_dataset, predictions, question_key=\"question\", prediction_key=\"result\")\n",
    "    scores_list.append(graded_outputs)\n",
    "    latency_list.append(latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "stor=pd.DataFrame()\n",
    "\n",
    "for i,chunk_size in enumerate([\"GPT3.5_k_4\",\"GPT4_k_4\",\"GPT4_k_8\"]):\n",
    "    d=scores_list[i]\n",
    "    incorrect_counts = []\n",
    "    for dictionary in d:\n",
    "        if dictionary['text'] == 'INCORRECT':\n",
    "            incorrect_counts.append(1)\n",
    "        else:\n",
    "            incorrect_counts.append(0)\n",
    "    stor.loc[chunk_size,'num_incorrect']=sum(incorrect_counts)\n",
    "\n",
    "stor['pct_incorrect'] = stor['num_incorrect']  / len(eval_set)\n",
    "stor['pct_correct'] = 1 - stor['pct_incorrect']\n",
    "stor['pct_correct'].plot(kind='bar')\n",
    "plt.title('Percentage of Correct Answers')\n",
    "plt.xlabel('Chain')\n",
    "plt.ylabel('Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latency=pd.DataFrame(latency_list).T\n",
    "latency.columns = [\"GPT3.5_k_4\",\"GPT4_k_4\",\"GPT4_k_8\"]\n",
    "latency.to_csv(\"results/latency.csv\")\n",
    "latency.boxplot()\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Latency per query (seconds)\")\n",
    "plt.title(\"Latency for QA comparing ChatGPT vs GPT4 \\n $\\mu$ per model = 4.7s,13.3s,19.1s, $N=52$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_summary(i):\n",
    "    d=pd.DataFrame(predictions_list[i])\n",
    "    d['score']=list(score[\"text\"] for score in scores_list[i])\n",
    "    return d\n",
    "\n",
    "GPT35_k_4_result=eval_summary(0)\n",
    "GPT4_k_4_result=eval_summary(1)\n",
    "GPT4_k_8_result=eval_summary(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT35_k_4_result.to_csv(\"results/GPT35_k_4_result.csv\")\n",
    "GPT4_k_4_result.to_csv(\"results/GPT4_k_4_result.csv\")\n",
    "GPT4_k_8_result.to_csv(\"results/GPT4_k_8_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong3_5=GPT35_k_4_result[GPT35_k_4_result.score != \"CORRECT\"]\n",
    "wrong3_5.to_csv(\"results/wrong3_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong4=GPT4_k_4_result[GPT4_k_4_result.score != \"CORRECT\"]\n",
    "wrong4.to_csv(\"results/wrong4_k4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong4=GPT4_k_4_result[GPT4_k_4_result.score != \"CORRECT\"]\n",
    "wrong4.to_csv(\"results/wrong4_k8.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
